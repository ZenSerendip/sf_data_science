{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7bfe54c-1f41-4856-8e3e-35a8bcd029c2",
   "metadata": {},
   "source": [
    "# <center> Практическое задание по теме \"Циклы\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cfcecd-f33d-448c-8a96-652d333794c5",
   "metadata": {},
   "source": [
    "<img src=https://1gai.ru/uploads/posts/2020-12/1608804716_54441.png width=300 align=\"right\">\n",
    "\n",
    "→ Поздравляем с освоением важных для анализа данных конструкций Python (переменные, структуры данных, условные операторы и циклы)!\n",
    "\n",
    "Настало время промежуточной практики, чтобы закрепить все полученные в предыдущих модулях навыки! Мы будем применять их на реальном проекте. Сегодня мы обратимся к классике: займемся анализом текстов на примере «Войны и мира» Льва Николаевича Толстого!\n",
    "\n",
    "В рамках практического кейса мы сначала вместе познакомимся с исходными данными и произведем некоторые манипуляции над ними. После чего вам предстоит самостоятельно выполнить несколько заданий на тему поиска наиболее значимых слов в тексте с помощью методов статистического анализа текста. \n",
    "\n",
    "> Итак, приступим!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e83d2f-728a-4ed1-9e1c-c7bdd5ace306",
   "metadata": {},
   "source": [
    "## <center> Знакомимся с данными"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cea00b-9902-4939-86c9-0a0fc27fc421",
   "metadata": {},
   "source": [
    "Текст произведения мы взяли в библиотеке [lib.ru](http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0073.shtml) и провели первоначальную обработку. Поскольку наша цель — обработка слов из этого произведения, мы разбили текст на слова и вывели каждое слово в отдельной строке. Кроме того, в местах, где начинаются главы, мы вывели строку `\"[new chapter]\"`.\n",
    "\n",
    "> Исходный текстовый файл хранится в общем доступе и находится [здесь](https://raw.githubusercontent.com/SkillfactoryDS/Datasets/master/war_peace_processed.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98561d8-c417-425a-be5a-c72b48c08e7d",
   "metadata": {},
   "source": [
    "Для начала скачаем текст книги по ссылке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96854932-b328-444f-ac37-b0a82103737a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'в', 'два', 'раза', 'короче', 'и', 'в', 'пять', 'раз', 'интереснее', '2', 'почти', 'нет', 'философических', 'отступлений', '3', 'в', 'сто', 'раз', 'легче', 'читать', 'весь', 'французский', 'текст', 'заменен', 'русским', 'в', 'переводе', 'самого', 'толстого', '4', 'гораздо', 'больше', 'мира', 'и', 'меньше', 'войны', '5', 'хеппи-энд', 'эти', 'слова', 'я', 'поместил', 'семь', 'лет', 'назад', 'на', 'обложку', 'предыдущего', 'издания', 'указав', 'в', 'аннотации', 'первая', 'полная', 'редакция', 'великого', 'романа', 'созданная', 'к', 'концу', '1866', 'года', 'до', 'того', 'как', 'толстой', 'переделал', 'его', 'в', '1867--1869', 'годах', '--', 'и', 'что', 'я', 'использовал', 'такие-то', 'публикации', 'думая', 'что', 'все', 'всё', 'знают', 'я', 'не', 'объяснил', 'откуда', 'взялась', 'эта', 'первая', 'редакция', 'я', 'оказался', 'неправ', 'и', 'в', 'результате', 'оголтелые', 'и']\n"
     ]
    }
   ],
   "source": [
    "# Импортируем библиотеку для выполнения HTTP-запросов в интернет\n",
    "import requests \n",
    "\n",
    "# Читаем текстовый файл по url-ссылке\n",
    "data = requests.get(\"https://raw.githubusercontent.com/SkillfactoryDS/Datasets/master/war_peace_processed.txt\").text\n",
    "\n",
    "# Предобрабатываем текстовый файл\n",
    "data = data.split('\\n')\n",
    "data.remove('')\n",
    "data = data + ['[new chapter]']\n",
    "\n",
    "# Выводим первые 100 слов из книги\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c227afc-419a-4612-9c76-e7888c6ba9e1",
   "metadata": {},
   "source": [
    "## <center> Работаем с данными"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed341b32-5b6c-4cf4-962b-350a6f85a599",
   "metadata": {},
   "source": [
    "Для начала найдем общее количество слов и количество уникальных слов в тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "518e0ec6-093f-44a7-ab7d-b34d47cb066c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общее количество слов: 300080\n",
      "Общее количество уникальных слов: 38210\n"
     ]
    }
   ],
   "source": [
    "# Превращаем список в множество, удаляя дублирующиеся слова\n",
    "word_set = set(data)\n",
    "# Удаляем из множества слово, символизирующее раздел между главами\n",
    "word_set.discard('[new chapter]')\n",
    "# Выводим результаты\n",
    "print('Общее количество слов: {}'.format(len(data)))\n",
    "print('Общее количество уникальных слов: {}'.format(len(word_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b05aff-1efa-41d0-b1d9-8484d16d7100",
   "metadata": {},
   "source": [
    "Давайте напишем программу, которая посчитает частоту каждого слова. Для этого создадим словарь, ключами которого будут являться слова, а значения - количество вхождений этого слова в текст произведения. Заодно подсчитаем количество глав"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1ff1fbb-5911-4b16-8598-8c169e60efc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество глав: 171\n",
      "1 10\n",
      "в 6997\n",
      "два 215\n",
      "раза 35\n",
      "короче 3\n",
      "и 14592\n",
      "пять 61\n",
      "раз 296\n",
      "интереснее 4\n",
      "2 12\n"
     ]
    }
   ],
   "source": [
    "# Инициализируем пустой словарь\n",
    "word_counts = {}\n",
    "# Инициализируем количество глав\n",
    "count_chapter = 0\n",
    "# Создаем цикл по всем словам из списка слов\n",
    "for word in data:\n",
    "    # Проверяем, что текущее слово - обозначение новой главы\n",
    "    if word == '[new chapter]':\n",
    "        # Если условие выполняется, то увеличиваем количество глав на 1\n",
    "        count_chapter += 1\n",
    "        # Переходим на новую итерацию цикла\n",
    "        continue\n",
    "    # Проверяем, что текущего слова еще нет в словаре слов\n",
    "    if word not in word_counts:\n",
    "        # Если условие выполняется, инициализируем новый ключ 1\n",
    "        word_counts[word] = 1\n",
    "    else:\n",
    "        # В противном случае, увеличиваем количество слов на 1\n",
    "        word_counts[word] += 1\n",
    "\n",
    "# Выводим количество глав\n",
    "print('Количество глав: {}'.format(count_chapter))\n",
    "\n",
    "# Создаем цикл по ключам и их порядковым номерам полученного словаря\n",
    "for i, key in enumerate(word_counts):\n",
    "    # Выводим только первые 10 слов\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(key, word_counts[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b6fe96-1662-4f22-ba4a-1a786f457884",
   "metadata": {},
   "source": [
    "Разделим все слова на главы. Для этого создадим список, в котором будем хранить списки - слова из определенной главы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52d5f481-1195-47cd-9466-bbafb273e290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вложенный список содержит 171 внутренних списка\n",
      "['1', 'в', 'два', 'раза', 'короче', 'и', 'в', 'пять', 'раз', 'интереснее', '2', 'почти', 'нет', 'философических', 'отступлений', '3', 'в', 'сто', 'раз', 'легче', 'читать', 'весь', 'французский', 'текст', 'заменен', 'русским', 'в', 'переводе', 'самого', 'толстого', '4', 'гораздо', 'больше', 'мира', 'и', 'меньше', 'войны', '5', 'хеппи-энд', 'эти', 'слова', 'я', 'поместил', 'семь', 'лет', 'назад', 'на', 'обложку', 'предыдущего', 'издания', 'указав', 'в', 'аннотации', 'первая', 'полная', 'редакция', 'великого', 'романа', 'созданная', 'к', 'концу', '1866', 'года', 'до', 'того', 'как', 'толстой', 'переделал', 'его', 'в', '1867--1869', 'годах', '--', 'и', 'что', 'я', 'использовал', 'такие-то', 'публикации', 'думая', 'что', 'все', 'всё', 'знают', 'я', 'не', 'объяснил', 'откуда', 'взялась', 'эта', 'первая', 'редакция', 'я', 'оказался', 'неправ', 'и', 'в', 'результате', 'оголтелые', 'и']\n"
     ]
    }
   ],
   "source": [
    "# Инициализируем общий список, в котором будем хранить списки слов в каждой главе\n",
    "chapter_data = []\n",
    "# Инициализируем список слов, в котором будет хранить слова одной главы\n",
    "chapter_words = []\n",
    "\n",
    "# Создаем цикл по всем словам из списка\n",
    "for word in data:\n",
    "    # Проверяем, что текущее слово - обозначение новой главы\n",
    "    if word == '[new chapter]':\n",
    "        # Если условие выполняется, добавляем список со словами из главы в общий список\n",
    "        chapter_data.append(chapter_words)\n",
    "        # Обновляем (перезаписываем) список со словами из текущей главы\n",
    "        chapter_words = []\n",
    "    else:\n",
    "        # В противном случае, добавляем текущее слово в список со словами из главы\n",
    "        chapter_words.append(word)\n",
    "\n",
    "# Проверяем, что у нас получилось столько же списков, сколько глав в произведении\n",
    "print('Вложенный список содержит {} внутренних списка'.format(len(chapter_data)))\n",
    "# Выведем первые 100 слов 0-ой главы\n",
    "print(chapter_data[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "486f7383-4536-4d04-9b56-da3b6731c5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'в'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapter_data[15][100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a6ad5f-bd35-463f-964e-ec7679094712",
   "metadata": {},
   "source": [
    "Подсчитаем, сколько раз каждое слово встречается в каждой из глав"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c89ab300-3aac-465a-9854-9317647aab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем список, в котором будем хранить словари\n",
    "chapter_words_count = []\n",
    "\n",
    "# Создаем цикл по элементам внешнего списка со словами\n",
    "for chapter_words in chapter_data:\n",
    "    # Инициализируем пустой словарь, куда будем добавлять результаты\n",
    "    temp = {}\n",
    "    # Создаем цикл по элементам внутреннего списка\n",
    "    for word in chapter_words:\n",
    "        # Проверяем, что текущего слова еще нет в словаре\n",
    "        if word not in temp:\n",
    "            # Если условие выполняется, добавляем ключ в словарь\n",
    "            temp[word] = 1\n",
    "        else:\n",
    "            # В противном случае, увеличиваем количество влождений слова в главу\n",
    "            temp[word] += 1\n",
    "    # Добавляем получившийся словарь в список\n",
    "    chapter_words_count.append(temp)\n",
    "\n",
    "# Выводим результат\n",
    "#print(chapter_words_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f554027e-36e5-485d-84bb-c93d24e5d37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapter_words_count[15]['князю']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2d4e15e-f5a5-4ba0-aeb8-ede29f853252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Chapter: 0\n",
      "----------------------------------------\n",
      "1 1\n",
      "в 37\n",
      "два 3\n",
      "раза 1\n",
      "короче 1\n",
      "и 34\n",
      "пять 1\n",
      "раз 2\n",
      "интереснее 1\n",
      "2 1\n",
      "----------------------------------------\n",
      "Chapter: 1\n",
      "----------------------------------------\n",
      "автора 1\n",
      "я 20\n",
      "пишу 2\n",
      "до 1\n",
      "сих 1\n",
      "пор 1\n",
      "только 5\n",
      "о 2\n",
      "князьях 1\n",
      "графах 1\n",
      "----------------------------------------\n",
      "Chapter: 2\n",
      "----------------------------------------\n",
      "первая 1\n",
      "----------------------------------------\n",
      "Chapter: 3\n",
      "----------------------------------------\n",
      "-- 81\n",
      "ну 5\n",
      "что 44\n",
      "князь 21\n",
      "генуя 1\n",
      "и 94\n",
      "лукка 1\n",
      "стали 1\n",
      "не 57\n",
      "больше 2\n",
      "----------------------------------------\n",
      "Chapter: 4\n",
      "----------------------------------------\n",
      "гостиная 1\n",
      "анны 2\n",
      "павловны 2\n",
      "начала 1\n",
      "понемногу 1\n",
      "наполняться 1\n",
      "приехала 3\n",
      "высшая 1\n",
      "знать 1\n",
      "петербурга 2\n"
     ]
    }
   ],
   "source": [
    "# Создаем цикл по ключам словаря - спискам слов и их порядковым номерам\n",
    "for chapter_number, chapter_dict in enumerate(chapter_words_count):\n",
    "    # Выводим только первые 5 глав\n",
    "    if chapter_number == 5:\n",
    "        break\n",
    "    # Выводим номер главы\n",
    "    print('-' * 40)\n",
    "    print('Chapter: {}'.format(chapter_number))\n",
    "    print('-' * 40)\n",
    "    # Создаем цикл по ключам - словам и их порядковым номерам\n",
    "    for j, word in enumerate(chapter_dict):\n",
    "        # Выводим первые 10 слов из главы\n",
    "        if j == 10:\n",
    "            break\n",
    "        print(word, chapter_dict[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf264f8d-35e5-4f46-b788-baa074dfefd9",
   "metadata": {},
   "source": [
    "Давайте резюмировать, что мы с вами уже получили:\n",
    "\n",
    "* `word_set` - множество из всех слов, которые есть в книге\n",
    "\n",
    "* `count_chapter` - количество глав в книге (171)\n",
    "\n",
    "* `word_counts` - словарь, ключами которого являются слова, а значениями - количество вхождений этих слов в книгу\n",
    "\n",
    "* `chapter_data` - список из 171 списка, где элементы вложенных списков - все слова из главы. Каждый список соответствует своей главе\n",
    "\n",
    "* `chapter_words_count` - список из 171 словаря, где ключи - слова, а значения - количество слов в главе. Каждый словарь соответствует своей главе\n",
    "\n",
    "Учтите, что эти данные могут пригодиться вам при выполнении дальнейших заданий.\n",
    "\n",
    "> А теперь к заданиями!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9425c84b-9f68-4dd9-a3f5-e0eb24ec0c86",
   "metadata": {},
   "source": [
    "## <center> Задания для самостоятельного решения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648dc36c-9163-4a78-8191-27e5d7d766b9",
   "metadata": {},
   "source": [
    "### Задание 1.\n",
    "\n",
    "Давайте введем понятие частоты употребления отдельного слова в документе (`term frequency`, или `tf`). В нашем случае речь идёт не о документах, а о главах книги (выше мы писали, что в текстовом документе главы разделяются строкой '[new chapter]').\n",
    "\n",
    "Формула для вычисления `term frequency` для слова `word`:\n",
    "$$ tf_{word, chapter} = \\frac {n_{word, chapter}} {n_{chapter}}$$\n",
    "\n",
    "где \n",
    "* ${n_{word, chapter}}$ - сколько раз слово `word` встрачается в главе `chapter`, \n",
    "* $n_{chapter}$ - количество слов в главе `chapter`.\n",
    "\n",
    "\n",
    "Например, слово `\"гостья\"` употребляется в 15-ой главе 10 раз (${n_{word, chapter}}$).(кстати, главы у нас нумеруются с 0). Общее количество слов в тексте 15-ой главы - 1359 ($n_{chapter}$). Тогда:\n",
    "\n",
    "$$ tf_{гостья, 15} = \\frac{10}{1359} \\approx 0.007358$$\n",
    "\n",
    "**Задание:** \n",
    "\n",
    "Напишите программу, которая позволит получать частоту употребления любого заданного слова `target_word` в заданной главе `target_chapter`. \n",
    "\n",
    "**Дополнительное требование:**\n",
    "\n",
    "*Пострайтесь сделать программу максимально обобщенной. То есть желательно рассчитать характеристику `tf` для всех слов из каждой главы, чтобы впоследствии не было необходимости производить вычисления снова.*\n",
    "\n",
    "**Подсказка:**\n",
    "\n",
    "*Для этого вы можете для каждой главы создать словарь, ключами которого являются слова, а значения - частота употребления этого слова в этой главе*\n",
    "\n",
    "**Протестируйте работу программы на нескольких словах и главах.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3d29b2a4-7b47-438e-b3b2-3d1b9b9520ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007358\n"
     ]
    }
   ],
   "source": [
    "# Задание 1.\n",
    "\n",
    "target_word = 'гостья'\n",
    "target_chapter = 15\n",
    "\n",
    "all_words_tf = [] # список из словарей для слов с частотной характеристикой\n",
    "for num_chapter in range(count_chapter):\n",
    "    n_words = len(chapter_data[num_chapter]) # кол-во слов в текущей главе\n",
    "    tf_dict = {} # создаем, а потом в цикле, очщаем словарь\n",
    "    for word, count in chapter_words_count[num_chapter].items(): # слово и его кол-во в текущей главе\n",
    "        tf_dict[word] = count / n_words # добавляем в словарь частотную характеристику каждого слова\n",
    "    all_words_tf.append(tf_dict) # добавляем словари в список\n",
    "    \n",
    "# вычислим частоту повторения искомого слова в главе\n",
    "tf = round(all_words_tf[target_chapter][target_word] , 6)\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843d4785-89ee-4437-8cc7-daf52a800c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "876f2ef2-e4e7-4a4f-be3b-eac18c2bdc62",
   "metadata": {},
   "source": [
    "### Задание 2.\n",
    "\n",
    "Пришло время познакомиться с понятием `document frequency`.\n",
    "\n",
    "`Document frequency` (для удобства сократим до `df`) — это доля документов, в которых встречается искомое слово. \n",
    "\n",
    "Вычисляется по формуле:\n",
    "\n",
    "$$ df_{word} = \\frac{N_{word}}{N} $$, \n",
    "\n",
    "где \n",
    "* $N_{word}$ - число документов (глав) содержащих слово `word`, \n",
    "* $N$ - общее число документов (глав).\n",
    "\n",
    "Объясним на примере: наш текст состоит из 171 главы ($N$), а слово `\"человек\"` встречается в 115 главах. Тогда:\n",
    "\n",
    "$$ df_{человек} = \\frac{115}{171} \\approx 0.6725$$\n",
    "\n",
    "**Задание:** \n",
    "\n",
    "Напишите программу, которая позволит вычислять document frequency для заданного слова `target_word` и выведить результат на экран.\n",
    "\n",
    "**Дополнительное требование:**\n",
    "\n",
    "*Пострайтесь сделать программу максимально обобщенной. То есть желательно рассчитать характеристику `df` для всех уникальных слов из книги, чтобы впоследствии не было необходимости производить вычисления снова.*\n",
    "\n",
    "**Подсказка:**\n",
    "*Для этого вы можете создать словарь, ключами которого являются слова из книги, а значения - доля документов, содержащих эти слова*\n",
    "\n",
    "**Протестируйте работу программы на нескольких словах** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b704db9d-d353-48bf-9805-527f82f6f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь\n",
    "# word_set - множество из всех слов, которые есть в книге\n",
    "# count_chapter - количество глав в книге (171)\n",
    "# word_counts - словарь, ключами которого являются слова, а значениями - количество вхождений этих слов в книгу\n",
    "# chapter_data - список из 171 списка, где элементы вложенных списков - все слова из главы. Каждый список соответствует своей главе\n",
    "# chapter_words_count - список из 171 словаря, где ключи - слова, а значения - количество слов в главе. Каждый словарь соответствует своей главе\n",
    "   \n",
    "word_in_chapters = {} # словарь частотных хапрактеристик для всех слов в главах\n",
    "for word in word_set: # перебирааем все слова из книги\n",
    "    count = 0 # инициализируем и потом обнуляем в пследующем цикле счетчик для подсчета найденных сдлв\n",
    "    for num_chapter in range(count_chapter): # проебагемся по всем главам   \n",
    "        count += word in chapter_data[num_chapter] # если слово существует в списке всех лов главы, увеличиваем счетчик\n",
    "    word_in_chapters[word] = count # записываем в ловарь кол-во найденных слов    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b1af679a-3b49-4aca-8e84-bd206875fbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.672515\n"
     ]
    }
   ],
   "source": [
    "target_word = 'человек' \n",
    "\n",
    "# найдем долю документов, в которых содержится искомое слово\n",
    "df = round(word_in_chapters[target_word] / count_chapter, 6)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d75c365-9dd4-494a-90f0-cb905a0b1883",
   "metadata": {},
   "source": [
    "### Задание 3\n",
    "\n",
    "Пришло время дать разъяснения: для чего мы делали вычисления выше и что нас ждет впереди?\n",
    "\n",
    "> Если какое-то слово часто употребляется в документе, то, вероятно, этот документ что-то рассказывает о предмете/действии, описываемом этим словом. Скажем, если вы читаете книгу, в которой много раз употребляется слово `\"заяц\"`, то, вероятно, эта книга про зайцев.\n",
    "\n",
    "> Однако, если вы возьмёте слово `\"и\"`, то оно будет встречаться почти в каждой книге много раз. \n",
    "\n",
    "Таким образом, если мы хотим найти наиболее значимые слова в книге, мы, с одной стороны, хотим найти наиболее частые слова, а с другой — убрать те, которые не несут важной информации, так как встречаются везде.\n",
    "\n",
    "Такая задача хорошо решается с помощью `tf-idf` — статистической метрики для оценки важности слова в тексте. Другими словами, `tf-idf` — это «контрастность» слова в документе (насколько оно выделяется среди других слов). \n",
    "\n",
    "Формула для вычисления следующая:\n",
    "\n",
    "`tf-idf = term frequency * inverse document frequency`\n",
    "\n",
    "* `tf` — это частотность термина, которая измеряет, насколько часто термин встречается в документе.\n",
    "\n",
    "* `idf` — это обратная документная частотность термина. Она измеряет непосредственно важность термина во всём множестве документов.\n",
    "\n",
    "Чтобы получить `idf`, необходимо поделить 1 на полученную в Задании 1 документную частоту (`df`):\n",
    "\n",
    "$$idf = \\frac{1}{df}$$\n",
    "\n",
    "Мы будем использовать не сырые значения `idf`, а их логарифмы, то есть $tf * log(idf)$. Сейчас мы не будем заострять внимания на том, почему следует использовать именно логарифм — это долгий разговор. Вернемся к нему, когда будем изучать методы машинного обучения для обработки текстов. Подробнее о `tf-idf` вы можете почитать [здесь](https://translated.turbopages.org/proxy_u/en-ru.ru.15518a02-63e76541-6895b80b-74722d776562/https/www.freecodecamp.org/news/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3/).\n",
    "\n",
    "В качестве примера измерим `tf-idf` слова `\"анна\"` в главе 4. Слово `\"анна\"` встречается в указанной главе 7 раз, при этом в 4 главе 1060 слов, всего же слово `\"анна\"` упоминается в 32 главах из 171.\n",
    "\n",
    "Таким образом, `tf-idf` данного слова в данной главе будет равно:\n",
    "\n",
    "$$tf\\_idf_{анна, 4} = tf * log(\\frac{1}{df}) = \\frac{7}{1060} * log(\\frac {171}{32}) \\approx 0.011067$$\n",
    "\n",
    "**Примечание:** здесь используется натуральный логарифм по основанию $e$, однако в общем случае основание логарифма не имеет значения, так как характеристика `tf-idf` используется для сравнения контрастности слов между собой\n",
    "\n",
    "**Задание**:\n",
    "\n",
    "Напишите программу, которая позволяет вычислять значение `tf-idf` для заданного слова `target_word` в заданной главе `target_chapter`.\n",
    "\n",
    "**Дополнительное требование:**\n",
    "\n",
    "*Пострайтесь сделать программу максимально оптимальной. То есть желательно рассчитать характеристику `tf-idf` для всех слов из каждой главы книги, чтобы впоследствии не было необходимости производить вычисления снова.*\n",
    "\n",
    "**Протестируйте работу программы на нескольких словах и главах.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d41bf24-05fd-489c-a5cb-f2a1a51e0280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8903717578961645\n"
     ]
    }
   ],
   "source": [
    "# Импортируем функцию log из модуля math:\n",
    "from math import log\n",
    "print(log(18))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d6300-72c8-41c1-92f7-b2bc0fb6ac46",
   "metadata": {},
   "source": [
    "**Примечание.** \n",
    "\n",
    "**Модуль (библиотека) в Python** — это любой программный файл, который содержит в себе код, включая функции. В нашем случае math — это встроенный модуль, содержащий функционал для математических вычислений. Подробнее о math вы можете почитать [здесь](https://pythonworld.ru/moduli/modul-math.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e9663ddb-5c00-4a7d-8bd9-2bac84cce243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задание 3\n",
    "\n",
    "# Ваш код здесь\n",
    "\n",
    "from math import log\n",
    "\n",
    "all_words_tf = [] # список из словарей для слов с частотной характеристикой\n",
    "for num_chapter in range(count_chapter):\n",
    "    n_words = len(chapter_data[num_chapter]) # кол-во слов в текущей главе\n",
    "    tf_dict = {} # создаем, а потом в цикле, очщаем словарь\n",
    "    for word, count in chapter_words_count[num_chapter].items(): # слово и его кол-во в текущей главе\n",
    "        tf_dict[word] = count / n_words # добавляем в словарь частотную характеристику каждого слова\n",
    "    all_words_tf.append(tf_dict) # добавляем словари в список\n",
    "\n",
    "    \n",
    "word_in_chapters = {} # словарь частотных хапрактеристик для всех слов в главах\n",
    "\n",
    "for word in word_set: # перебирааем все слова из книги\n",
    "    count = 0 # инициализируем и потом обнуляем в пследующем цикле счетчик для подсчета найденных слов\n",
    "    for num_chapter in range(count_chapter): # проебагемся по всем главам   \n",
    "        count += word in chapter_data[num_chapter] # если слово существует в списке всех слов главы, увеличиваем счетчик\n",
    "    word_in_chapters[word] = count # записываем в словарь кол-во найденных слов\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "94ec84bc-9bad-4bb8-8803-f61a05f97357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011067\n"
     ]
    }
   ],
   "source": [
    "target_word = 'анна'\n",
    "target_chapter = 4\n",
    "\n",
    "df = word_in_chapters[target_word] / count_chapter # найдем долю глав, в которых содержится искомое слово\n",
    "tf = all_words_tf[target_chapter][target_word] # вычислим частоту повторения искомого слова в главе\n",
    "tf_idf = round(tf * log(1 / df), 6) # вычислим «контрастность» заданного слова в заданной главе \n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e661da20-ed8e-4d23-b2b1-56b7015ef0b2",
   "metadata": {},
   "source": [
    "### Задание 4.\n",
    "\n",
    "Теперь, когда мы умеем вычислять `tf-idf` для каждого слова в главе, мы можем найти те слова, которые являются самыми «контрастными» для данной главы, то есть они могут являться в своём роде заголовком для главы.\n",
    "\n",
    "Например, для главы 3 наиболее значимыми словами будут:\n",
    "\n",
    "`\"анна\", \"павловна\", \"функе\"`\n",
    "\n",
    "**Задание:**\n",
    "\n",
    "Напишите программу, которая позволяет вывести три слова, имеющие самое высокое значение `tf-idf` в заданной главе `target_chapter` в порядке убывания `tf-idf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9be238a8-14ae-4343-9b99-01c1b76e271a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['павловна', 'анна', 'функе']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_chapter = 3\n",
    "\n",
    "# Ваш код здесь\n",
    "\n",
    "tf_idf_chapter = {}\n",
    "for word in chapter_data[target_chapter]:\n",
    "    df = word_in_chapters[word] / count_chapter # найдем долю глав, в которых содержится искомое слово\n",
    "    tf = all_words_tf[target_chapter][word] # вычислим частоту повторения искомого слова в главе\n",
    "    tf_idf = tf * log(1 / df) # вычислим «контрастность» заданного слова в заданной главе\n",
    "    tf_idf_chapter[word] = tf_idf\n",
    "\n",
    "# sorted_tf_idf_chapter = dict(sorted(tf_idf_chapter.items(), key=lambda item: item[1]))\n",
    "# sorted_tf_idf_chapter = sorted(tf_idf_chapter.values(), reverse = True)\n",
    "sorted_tf_idf_chapter = sorted(tf_idf_chapter, key=tf_idf_chapter.get, reverse = True) # отсортируем значения словаря в порядке убывания и вернем список ключей \n",
    "sorted_tf_idf_chapter[:3] # выводим первые три наиболее значимые слова"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
